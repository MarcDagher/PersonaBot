{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get environment variables\n",
    "dotenv_path = Path('./.env')\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "os.environ[\"NEO4J_URI\"] = os.getenv('uri')\n",
    "os.environ[\"NEO4J_USERNAME\"] = os.getenv('user_name')\n",
    "os.environ[\"NEO4J_PASSWORD\"] = os.getenv('password')\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing GraphCypherQAChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Query Knowledge Graph\n",
    "# from langchain.chains import GraphCypherQAChain\n",
    "# from langchain.graphs import Neo4jGraph\n",
    "# from langchain_groq import ChatGroq\n",
    "\n",
    "# # Connect to graph\n",
    "# graph = Neo4jGraph()\n",
    "\n",
    "# # Create LLM instance\n",
    "# llm = ChatGroq(temperature=0, groq_api_key=os.environ[\"GROQ_API_KEY\"], model_name=\"llama-3.1-70b-versatile\")\n",
    "\n",
    "# graph_schema = f\"Context of the graph's schema: {graph.schema}\"\n",
    "# personality_traits = graph.query(\"MATCH (n:Personality_Trait) return n\")\n",
    "# personality_traits = [title['n']['title'] for title in personality_traits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = f\"\"\"Your role is to recommend career paths for me. You have access to a Neo4j Graph Database. Here is the graph's schema: {graph_schema} and here the Personality_Trait titles that you can use: {personality_traits} and not that job_zone resembles the level of experience required for the job from 1 till 5 as integers. Use the graph to supplement your answer. Here are some details about myself: I love people and I am a good listener. I enjoy observation and analysis. what careers would you recommend?\"\"\"\n",
    "\n",
    "# chain = GraphCypherQAChain.from_llm(graph=graph, llm=llm, verbose=True, validate_cypher=True)\n",
    "# response = chain.invoke({\"query\": prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(response['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Tool Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_groq import ChatGroq\n",
    "# from langchain_core.pydantic_v1 import BaseModel\n",
    "# from graph_functions import connect_to_database\n",
    "\n",
    "# class QueryKnowledgeGraph(BaseModel):\n",
    "#     f'''The contains details about Occupations and their related demands. This is the schema of the graph as context: {label_and_prop, rel, rel_and_prop}. In your cypher query do not use any words that are not found in the schema. Use the graphs relations, labels, and properties to supplement your answer. Return an answer to the question, appraoch you used, the node_id that you referred to, and the query you wrote.'''\n",
    "\n",
    "#     answer: str\n",
    "#     approach_used: str\n",
    "#     node_id: str\n",
    "#     cypher_query: str\n",
    "\n",
    "#     def query_neo4j(query):\n",
    "#         result = graph.query(query)\n",
    "#         return result\n",
    "\n",
    "\n",
    "# chat = ChatGroq(temperature=1, groq_api_key=os.environ[\"GROQ_API_KEY\"], model_name=\"mixtral-8x7b-32768\")\n",
    "# structured_llm = chat.with_structured_output(QueryKnowledgeGraph, include_raw=True)\n",
    "\n",
    "# answer = structured_llm.invoke(\"I love people and I love to listen, what would you recommend as a career?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(answer['parsed'].answer)\n",
    "# print(answer['parsed'].approach_used)\n",
    "# print(answer['parsed'].node_id)\n",
    "# print(answer['parsed'].cypher_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver = connect_to_database(uri=os.getenv('uri'), username = os.getenv('user_name'), password = os.getenv('password'))\n",
    "# query = f\"MATCH (n:Occupation)-[]-(m:`Personality Trait` {{title: 'Artistic'}}) RETURN n\"\n",
    "# # with driver.session() as session:\n",
    "# #     result = session.run(query)\n",
    "# #     result = result.value()\n",
    "# #     driver.close()\n",
    "# result = graph.query(query)\n",
    "# result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_groq import ChatGroq\n",
    "# from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# chat = ChatGroq(temperature=0, groq_api_key=os.environ[\"GROQ_API_KEY\"], model_name=\"mixtral-8x7b-32768\")\n",
    "\n",
    "# prompt = f\"The neo4j graph database contains details about Occupations and their related demands. This is the schema of the graph as context:\\n{label_and_prop}\\n{rel}\\n{rel_and_prop}\\nIn your cypher query do not use any words that are not found in the schema. Use the graphs relations, labels, and properties to supplement your answer. Return an answer to the question, appraoch you used, the node_id that you referred to, and the query you wrote.\"\n",
    "\n",
    "# prompt_template = ChatPromptTemplate.from_template(prompt)\n",
    "\n",
    "# response = chat.invoke(\"I love people and I love to listen, what would you recommend as a career?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding memory to a conversation\n",
    "\n",
    "from langchain.chains import ConversationChain # Depricated\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory\n",
    "\n",
    "llm = ChatGroq(temperature=0, groq_api_key=os.environ[\"GROQ_API_KEY\"], model_name=\"mixtral-8x7b-32768\")\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "conversation = ConversationChain(\n",
    "  llm=llm,\n",
    "  memory=memory,\n",
    "  verbose=True\n",
    ")\n",
    "\n",
    "# To manually load previous memory\n",
    "# memory.save_context({\"input\": \"Hi\"}, {\"output\": \"What's up\"})\n",
    "# memory.load_memory_variables({})\n",
    "# conversation.predict(input=\"Limit your answers to one line. Give me a  love poem.\")\n",
    "# print(memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## A chain of prompts returning one output from one input\n",
    "# from langchain.chains import SequentialChain, LLMChain\n",
    "\n",
    "# llm = ChatGroq(temperature=0.9, groq_api_key=os.environ[\"GROQ_API_KEY\"], model_name=\"llama-3.1-70b-versatile\")\n",
    "\n",
    "# # prompt template 1: translate to english\n",
    "# first_prompt = ChatPromptTemplate.from_template(\"Translate the following review to english:\\n\\n{Review}\")\n",
    "# chain_one = LLMChain(llm=llm, prompt=first_prompt, output_key=\"English_Review\") # chain 1: input= Review and output= English_Review\n",
    "\n",
    "# second_prompt = ChatPromptTemplate.from_template(\"Can you summarize the following review in 1 sentence:\\n\\n{English_Review}\")\n",
    "# chain_two = LLMChain(llm=llm, prompt=second_prompt, output_key=\"summary\") # chain 2: input= English_Review and output= summary\n",
    "\n",
    "# # prompt template 3: translate to english\n",
    "# third_prompt = ChatPromptTemplate.from_template(\"What language is the following review:\\n\\n{Review}\")\n",
    "# chain_three = LLMChain(llm=llm, prompt=third_prompt, output_key=\"language\") # chain 3: input= Review and output= language\n",
    "\n",
    "# # prompt template 4: follow up message\n",
    "# fourth_prompt = ChatPromptTemplate.from_template(\"Write a follow up response to the following summary in the specified language:\\n\\nSummary: {summary}\\n\\nLanguage: {language}\")\n",
    "# chain_four = LLMChain(llm=llm, prompt=fourth_prompt, output_key=\"followup_message\") # chain 4: input= summary, language and output= followup_message\n",
    "\n",
    "# # overall_chain: input= Review \n",
    "# # and output= English_Review,summary, followup_message\n",
    "# overall_chain = SequentialChain(\n",
    "#     chains=[chain_one, chain_two, chain_three, chain_four],\n",
    "#     input_variables=[\"Review\"], \n",
    "#     output_variables=[\"English_Review\", \"summary\",\"followup_message\"], \n",
    "#     verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = overall_chain.invoke(\"Ana b7ebbak men kel albe ya kbir\")\n",
    "\n",
    "# print(\"review => \", output['Review'], '\\n')\n",
    "# print(\"english review => \", output['English_Review'], '\\n')\n",
    "# print(\"summary => \", output['summary'], '\\n')\n",
    "# print(\"followup message => \", output['followup_message'], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversational Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Get environment variables\n",
    "dotenv_path = Path('./.env')\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "os.environ[\"NEO4J_URI\"] = os.getenv('uri')\n",
    "os.environ[\"NEO4J_USERNAME\"] = os.getenv('user_name')\n",
    "os.environ[\"NEO4J_PASSWORD\"] = os.getenv('password')\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Query Knowledge Graph\n",
    "from langchain.chains import GraphCypherQAChain\n",
    "from langchain.graphs import Neo4jGraph\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Connect to graph\n",
    "graph = Neo4jGraph()\n",
    "\n",
    "# Create LLM instance\n",
    "llm = ChatGroq(temperature=0, groq_api_key=os.environ[\"GROQ_API_KEY\"], model_name=\"llama-3.1-70b-versatile\")\n",
    "\n",
    "# Prepare graph schema and context of Personality_Trait(s)\n",
    "graph_schema = f\"Context of the graph's schema: {graph.schema}\"\n",
    "personality_traits = graph.query(\"MATCH (n:Personality_Trait) return n\")\n",
    "personality_traits = [title['n']['title'] for title in personality_traits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare initial prompt\n",
    "prompt = f\"\"\"Your role is to recommend career paths for me. You have access to a Neo4j Graph Database. Here is the graph's schema: {graph_schema} and here the Personality_Trait titles that you can use: {personality_traits} and not that job_zone resembles the level of experience required for the job from 1 till 5 as integers. Use the graph to supplement your answer. Here are some details about myself: I love people and I am a good listener. I enjoy observation and analysis. what careers would you recommend?\"\"\"\n",
    "\n",
    "# chain that queries the graph\n",
    "chain = GraphCypherQAChain.from_llm(graph=graph, llm=llm, verbose=True, validate_cypher=True)\n",
    "response = chain.invoke({\"query\": prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for RunnableWithMessageHistory\nget_session_history\n  True is not callable (type=type_error.callable; value=True)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 18\u001b[0m\n\u001b[0;32m     12\u001b[0m memory \u001b[38;5;241m=\u001b[39m ConversationSummaryMemory(llm\u001b[38;5;241m=\u001b[39mllm)\n\u001b[0;32m     14\u001b[0m conversation_chain \u001b[38;5;241m=\u001b[39m ConversationChain(llm\u001b[38;5;241m=\u001b[39mllm, memory\u001b[38;5;241m=\u001b[39mmemory, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 18\u001b[0m chain \u001b[38;5;241m=\u001b[39m \u001b[43mRunnableWithMessageHistory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_template\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_session_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m result \u001b[38;5;241m=\u001b[39m conversation_chain\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "File \u001b[1;32mc:\\Users\\Dagher\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\runnables\\history.py:351\u001b[0m, in \u001b[0;36mRunnableWithMessageHistory.__init__\u001b[1;34m(self, runnable, get_session_history, input_messages_key, output_messages_key, history_messages_key, history_factory_config, **kwargs)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;66;03m# If not provided, then we'll use the default session_id field\u001b[39;00m\n\u001b[0;32m    340\u001b[0m     _config_specs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    341\u001b[0m         ConfigurableFieldSpec(\n\u001b[0;32m    342\u001b[0m             \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msession_id\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    348\u001b[0m         ),\n\u001b[0;32m    349\u001b[0m     ]\n\u001b[1;32m--> 351\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    352\u001b[0m     get_session_history\u001b[38;5;241m=\u001b[39mget_session_history,\n\u001b[0;32m    353\u001b[0m     input_messages_key\u001b[38;5;241m=\u001b[39minput_messages_key,\n\u001b[0;32m    354\u001b[0m     output_messages_key\u001b[38;5;241m=\u001b[39moutput_messages_key,\n\u001b[0;32m    355\u001b[0m     bound\u001b[38;5;241m=\u001b[39mbound,\n\u001b[0;32m    356\u001b[0m     history_messages_key\u001b[38;5;241m=\u001b[39mhistory_messages_key,\n\u001b[0;32m    357\u001b[0m     history_factory_config\u001b[38;5;241m=\u001b[39m_config_specs,\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    359\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Dagher\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\runnables\\base.py:5019\u001b[0m, in \u001b[0;36mRunnableBindingBase.__init__\u001b[1;34m(self, bound, kwargs, config, config_factories, custom_input_type, custom_output_type, **other_kwargs)\u001b[0m\n\u001b[0;32m   4987\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m   4988\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4989\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4998\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mother_kwargs: Any,\n\u001b[0;32m   4999\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5000\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a RunnableBinding from a Runnable and kwargs.\u001b[39;00m\n\u001b[0;32m   5001\u001b[0m \n\u001b[0;32m   5002\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5017\u001b[0m \u001b[38;5;124;03m        **other_kwargs: Unpacked into the base class.\u001b[39;00m\n\u001b[0;32m   5018\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5019\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m   5020\u001b[0m         bound\u001b[38;5;241m=\u001b[39mbound,\n\u001b[0;32m   5021\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs \u001b[38;5;129;01mor\u001b[39;00m {},\n\u001b[0;32m   5022\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig \u001b[38;5;129;01mor\u001b[39;00m {},\n\u001b[0;32m   5023\u001b[0m         config_factories\u001b[38;5;241m=\u001b[39mconfig_factories \u001b[38;5;129;01mor\u001b[39;00m [],\n\u001b[0;32m   5024\u001b[0m         custom_input_type\u001b[38;5;241m=\u001b[39mcustom_input_type,\n\u001b[0;32m   5025\u001b[0m         custom_output_type\u001b[38;5;241m=\u001b[39mcustom_output_type,\n\u001b[0;32m   5026\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mother_kwargs,\n\u001b[0;32m   5027\u001b[0m     )\n\u001b[0;32m   5028\u001b[0m     \u001b[38;5;66;03m# if we don't explicitly set config to the TypedDict here,\u001b[39;00m\n\u001b[0;32m   5029\u001b[0m     \u001b[38;5;66;03m# the pydantic init above will strip out any of the \"extra\"\u001b[39;00m\n\u001b[0;32m   5030\u001b[0m     \u001b[38;5;66;03m# fields even though total=False on the typed dict.\u001b[39;00m\n\u001b[0;32m   5031\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config \u001b[38;5;129;01mor\u001b[39;00m {}\n",
      "File \u001b[1;32mc:\\Users\\Dagher\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\load\\serializable.py:113\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[1;32m--> 113\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Dagher\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pydantic\\v1\\main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[0;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[1;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for RunnableWithMessageHistory\nget_session_history\n  True is not callable (type=type_error.callable; value=True)"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# chain 1: Greet User and Explain what we will be doing\n",
    "# LLMChain: replaced by |\n",
    "system_template = \"You are a career guide. You will ask peronality questions based on RIASC model then based on the answers you will suggest career paths suitable for my answers. First, greet the user and explain the flow of what we will do.\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([('system', system_template), ('user', '{text}')])\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "conversation_chain = ConversationChain(llm=llm, memory=memory, verbose=True)\n",
    "\n",
    "\n",
    "\n",
    "chain = RunnableWithMessageHistory(prompt_template | llm, get_session_history=True)\n",
    "result = conversation_chain.invoke(\"Hello\")\n",
    "print(result)\n",
    "\n",
    "\n",
    "# chain 2: Ask questions\n",
    "\n",
    "\n",
    "## Conditions\n",
    "# chain 3a If user asks question => reply to user and then continue asking\n",
    "\n",
    "# chain 3b If questions are done => query graph\n",
    "\n",
    "# chain 4 Interpret answers and graph's query then return suggestions\n",
    "\n",
    "# chian 5 Flow doesn't end here, user can ask more questions.\n",
    "\n",
    "\n",
    "\n",
    "#############\n",
    "## Chains\n",
    "\n",
    "# LLMCheckerChain: This chain uses a second LLM call to verify its initial answer. Use this when you have an extra layer of validation on the initial LLM call.\n",
    "\n",
    "# LLMRouterChain: This chain uses an LLM to route between potential options.\n",
    "# MultiPromptChain: This chain routes input between multiple prompts. Use this when you have multiple potential prompts you could use to respond and want to route to just one.\n",
    "# MultiRetrievalQAChain: This chain uses an LLM to route input questions to the appropriate retriever for question answering.\n",
    "\n",
    "## Retreiver\n",
    "#  Self Query"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
