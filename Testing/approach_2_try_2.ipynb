{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach 2, second try:\n",
    "- Scientist receives conversation. \n",
    "- Goes through the conversational flow.\n",
    "- Queries the graph.\n",
    "- Save query in history of queries. \n",
    "- Save Query results in history of query results. \n",
    "- ToolMessage is not saved in conversation messages.\n",
    "- Have a variable for the extracted occupations or traits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from langchain.graphs import Neo4jGraph\n",
    "from langchain_core.tools import tool\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# LangGraph\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# LangSmith\n",
    "from langsmith import traceable\n",
    "\n",
    "# General Imports\n",
    "import os\n",
    "import ast\n",
    "import prompts\n",
    "import operator\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from typing import TypedDict, Annotated # to construct the agent's state\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Connect to graph\n",
    "dotenv_path = Path('../.env')\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "os.environ[\"NEO4J_URI\"] = os.getenv('NEO4J_URI')\n",
    "os.environ[\"NEO4J_USERNAME\"] = os.getenv('NEO4J_USERNAME')\n",
    "os.environ[\"NEO4J_PASSWORD\"] = os.getenv('NEO4J_PASSWORD')\n",
    "graph = Neo4jGraph()\n",
    "\n",
    "# Create the tool to be used by the Agent\n",
    "@tool\n",
    "def query_graph(query):\n",
    "  \"\"\"Query from Neo4j knowledge graph using Cypher.\"\"\"\n",
    "  return graph.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Agent's State\n",
    "class AgentState(TypedDict):\n",
    "    conversation: Annotated[list[ AnyMessage ], operator.add]\n",
    "    good_cypher_and_outputs: Annotated[dict[ str, str ], operator.or_]\n",
    "    bad_cypher: Annotated[list[ str ], operator.add]\n",
    "    extracted_data: Annotated[list[ str ], operator.add]\n",
    "\n",
    "    # existing_cyphers: list[str]\n",
    "    graph_data_to_be_used: list[str]\n",
    "\n",
    "# Create Agent\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, model, tools, system):\n",
    "        self.system = system\n",
    "        self.tools = {t.name: t for t in tools} # Save the tools' names that can be used\n",
    "        self.model = model.bind_tools(tools) # Provide the name of the tools to the agent\n",
    "\n",
    "        graph = StateGraph(AgentState) # Initialize a stateful graph\n",
    "        memory = MemorySaver()\n",
    "\n",
    "        graph.add_node(\"personality_scientist\", self.call_groq)\n",
    "        graph.add_node(\"validate_cypher_then_query_graph\", self.validate_cypher_then_query_graph) # Checks if query is new\n",
    "        graph.add_node(\"extract_data\", self.extract_data)\n",
    "        graph.add_node(\"recommend_careers\", self.recommend_careers)\n",
    "\n",
    "        graph.add_conditional_edges(\"personality_scientist\", self.validate_tool_call, {True: 'validate_cypher_then_query_graph', False: END})\n",
    "        graph.add_edge(\"validate_cypher_then_query_graph\", \"extract_data\")\n",
    "        graph.add_edge(\"extract_data\", \"recommend_careers\")\n",
    "        graph.add_edge(\"recommend_careers\", END)\n",
    "        graph.set_entry_point(\"personality_scientist\")\n",
    "\n",
    "        # Build graph\n",
    "        self.graph = graph.compile(checkpointer=memory)\n",
    "\n",
    "    ## Get the LLM's response and update the Agent's State by adding the response to the messages\n",
    "    @traceable\n",
    "    def call_groq(self, state: AgentState):\n",
    "        messages = state['conversation']\n",
    "        \n",
    "        conversation = [SystemMessage(content=self.system)] + messages\n",
    "        ai_response = self.model.invoke(conversation)\n",
    "\n",
    "        return {'conversation': [ai_response]}\n",
    "    \n",
    "    ## Check if the model called for an action by checking the last message in the conversation\n",
    "    def validate_tool_call(self, state: AgentState):\n",
    "        ai_message = state['conversation'][-1]\n",
    "        return len(ai_message.tool_calls) > 0\n",
    "    \n",
    "    ## Check if the new cypher codes have already been written by the LLM\n",
    "    def validate_cypher_then_query_graph(self, state: AgentState):\n",
    "        print(f\"\\n------- in validate query\")\n",
    "        tool_calls = state['conversation'][-1].tool_calls\n",
    "\n",
    "        good_cypher_and_outputs = {} # stores the cypher queries that returned an output\n",
    "        bad_cypher = [] # stores the cypher queries that did not return an output\n",
    "        graph_data_to_be_used = [] # stores the queries that the model currently wants to use\n",
    "\n",
    "        # Go over the cypher codes given by the LLM\n",
    "        for i in range(len(tool_calls)):\n",
    "            current_tool_call = tool_calls[i]\n",
    "            new_cypher = current_tool_call['args']['query']\n",
    "\n",
    "            status = None\n",
    "\n",
    "            # Check if tool exists\n",
    "            if current_tool_call['name'] in self.tools:\n",
    "                \n",
    "                # LLM checks if the cypher query has already been made + if it returned an output\n",
    "                if status == None:\n",
    "                    print(f\"----- Checker 1\")\n",
    "                    good_cyphers = list(state['good_cypher_and_outputs'].keys())\n",
    "                    for i in range(len( good_cyphers )):\n",
    "                        key = good_cyphers[i]\n",
    "                        comparison = self.model.invoke(\n",
    "                            prompts.cypher_code_analyst_prompt.format(cypher_code_1=new_cypher, cypher_code_2=key, graph_schema=graph.structured_schema)\n",
    "                            )\n",
    "                        print(f\"\\n-------- {comparison.content}\")\n",
    "                        # If Query exists => Save cypher code to later access the output and give it to the model\n",
    "                        if comparison.content.lower() == \"true\":\n",
    "                            print(\"\\n----- Good Cyphers are similar, save the cypher\")\n",
    "                            graph_data_to_be_used.append(key)\n",
    "                            status = \"good_cypher\"\n",
    "                            break\n",
    "                \n",
    "                # LLM checks if the cypher query has already been made + if it didn't return an output\n",
    "                if status == None:\n",
    "                    print(f\"----- Checker 2\")\n",
    "                    for i in range(len(state['bad_cypher'])):\n",
    "                        comparison = self.model.invoke(\n",
    "                            prompts.cypher_code_analyst_prompt.format(\n",
    "                                cypher_code_1=new_cypher, cypher_code_2=state['bad_cypher'][i], graph_schema=graph.structured_schema)\n",
    "                            )\n",
    "                        # Query exists => LLM should fix the query\n",
    "                        if comparison.content.lower() == \"true\":\n",
    "                            print(\"---- Bad Cyphers are similar, have the LLM query again\")\n",
    "                            status = \"bad_cypher\"\n",
    "                            break\n",
    "\n",
    "                # If the cypher code hasn't been used before => query the graph\n",
    "                if status == None:\n",
    "                    print(f\"----- Checker 3\")\n",
    "                    query_output = self.tools[current_tool_call['name']].invoke(new_cypher)\n",
    "                    result = ToolMessage(content=str(query_output), name=current_tool_call['name'], tool_call_id=current_tool_call['id'])\n",
    "                    \n",
    "                    if result.content not in [\"\", None, '[]']:\n",
    "                        good_cypher_and_outputs[new_cypher] = result.content\n",
    "                        graph_data_to_be_used.append(new_cypher)\n",
    "                        print(\"----- Successfully queried graph\")\n",
    "\n",
    "                    else:\n",
    "                        bad_cypher.append(new_cypher)\n",
    "                        print(\"----- Bad query\")\n",
    "            else:\n",
    "                print(\"tool name not found in list of tools\")\n",
    "\n",
    "        # Save the data that we got in the AgentState\n",
    "        return_statement = {}\n",
    "        if len(good_cypher_and_outputs) > 0: return_statement['good_cypher_and_outputs'] = good_cypher_and_outputs\n",
    "        if len(bad_cypher) > 0: return_statement['bad_cypher'] = bad_cypher\n",
    "        if len(graph_data_to_be_used) > 0: return_statement['graph_data_to_be_used'] = graph_data_to_be_used\n",
    "\n",
    "        return return_statement\n",
    "    \n",
    "    ## Invoke tool\n",
    "    def extract_data(self, state: AgentState):\n",
    "        print('\\n-------> In extract data')\n",
    "\n",
    "        if len(state['graph_data_to_be_used']) > 0:\n",
    "            data_to_give_to_the_LLM = [{cypher: state['good_cypher_and_outputs'][cypher]} for cypher in state['graph_data_to_be_used']]\n",
    "            extracted_data = self.model.invoke(prompts.extractor_prompt.format(queried_data = data_to_give_to_the_LLM))\n",
    "            print(\"----- Data has been extracted\")\n",
    "            return {'extracted_data': [extracted_data.content]}\n",
    "\n",
    "        else:\n",
    "            print(\"No queries to be made\")\n",
    "            return\n",
    "\n",
    "    ## Generate final output\n",
    "    def recommend_careers(self, state: AgentState):\n",
    "        if len(state['graph_data_to_be_used']) > 0:\n",
    "            prompt = [SystemMessage(content=self.system)] + state['conversation']\n",
    "            prompt = prompt + [HumanMessage(content= prompts.recommender_prompt.format(extracted_data=state['extracted_data'][-1]))]\n",
    "            \n",
    "            recommended_careers = self.model.invoke(prompt)\n",
    "            print(\"ready for recommendation\")\n",
    "            return {'conversation': [recommended_careers]}\n",
    "        else:\n",
    "            print(\"no data to use\")\n",
    "            return\n",
    "            # give the model a prompt to redo its querying, or use its own data, or point the edge to the model again ... this is where the model will query again if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGroq(temperature=0.8, groq_api_key=os.environ[\"GROQ_API_KEY\"], model_name=\"llama-3.1-70b-versatile\")\n",
    "# model = ChatGroq(temperature=0.0, groq_api_key=os.environ[\"GROQ_API_KEY\"], model_name=\"llama3-70b-8192\")\n",
    "\n",
    "agent = Agent(model=model, tools=[query_graph], system=prompts.personality_scientist_prompt.format(schema=graph.structured_schema))\n",
    "\n",
    "# graph_png = Image(agent.graph.get_graph().draw_mermaid_png())\n",
    "# display(graph_png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Are these still present?\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '<function=query_graph\":{\"query\": \"MATCH (n:Occupation)-[]->(m:Personality_Trait) return n, m\"}</function>'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m thread \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfigurable\u001b[39m\u001b[38;5;124m'\u001b[39m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthread_id\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m}}\n\u001b[0;32m      2\u001b[0m output \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mstream({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconversation\u001b[39m\u001b[38;5;124m\"\u001b[39m: [HumanMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAre these still present?\u001b[39m\u001b[38;5;124m\"\u001b[39m)], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgraph_data_to_be_used\u001b[39m\u001b[38;5;124m'\u001b[39m: []}, thread, stream_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m output:\n\u001b[0;32m      5\u001b[0m     message[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconversation\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpretty_print()\n",
      "File \u001b[1;32mc:\\Users\\Dagher\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langgraph\\pregel\\__init__.py:997\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[0;32m    994\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m fut, task\n\u001b[0;32m    996\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[1;32m--> 997\u001b[0m \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minflight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    998\u001b[0m \u001b[38;5;66;03m# don't keep futures around in memory longer than needed\u001b[39;00m\n\u001b[0;32m    999\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m done, inflight, futures\n",
      "File \u001b[1;32mc:\\Users\\Dagher\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langgraph\\pregel\\__init__.py:1398\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[1;34m(done, inflight, step, timeout_exc_cls)\u001b[0m\n\u001b[0;32m   1396\u001b[0m             inflight\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[0;32m   1397\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[1;32m-> 1398\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m   1400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[0;32m   1401\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n\u001b[0;32m   1402\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m inflight:\n\u001b[0;32m   1403\u001b[0m         \u001b[38;5;66;03m# cancel all pending tasks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Dagher\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langgraph\\pregel\\executor.py:60\u001b[0m, in \u001b[0;36mBackgroundExecutor.done\u001b[1;34m(self, task)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdone\u001b[39m(\u001b[38;5;28mself\u001b[39m, task: concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mFuture) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 60\u001b[0m         \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m GraphInterrupt:\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;66;03m# This exception is an interruption signal, not an error\u001b[39;00m\n\u001b[0;32m     63\u001b[0m         \u001b[38;5;66;03m# so we don't want to re-raise it on exit\u001b[39;00m\n\u001b[0;32m     64\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mpop(task)\n",
      "File \u001b[1;32mc:\\Users\\Dagher\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mc:\\Users\\Dagher\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Dagher\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32mc:\\Users\\Dagher\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langgraph\\pregel\\retry.py:25\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[1;34m(task, retry_policy)\u001b[0m\n\u001b[0;32m     23\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Dagher\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\runnables\\base.py:2876\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   2874\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m   2875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2876\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2877\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2878\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32mc:\\Users\\Dagher\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langgraph\\utils.py:102\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m accepts_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc):\n\u001b[0;32m    101\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\n\u001b[1;32m--> 102\u001b[0m     ret \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32mc:\\Users\\Dagher\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langsmith\\run_helpers.py:609\u001b[0m, in \u001b[0;36mtraceable.<locals>.decorator.<locals>.wrapper\u001b[1;34m(langsmith_extra, *args, **kwargs)\u001b[0m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    608\u001b[0m     _on_run_end(run_container, error\u001b[38;5;241m=\u001b[39me)\n\u001b[1;32m--> 609\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    610\u001b[0m _on_run_end(run_container, outputs\u001b[38;5;241m=\u001b[39mfunction_result)\n\u001b[0;32m    611\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function_result\n",
      "File \u001b[1;32mc:\\Users\\Dagher\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langsmith\\run_helpers.py:606\u001b[0m, in \u001b[0;36mtraceable.<locals>.decorator.<locals>.wrapper\u001b[1;34m(langsmith_extra, *args, **kwargs)\u001b[0m\n\u001b[0;32m    604\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m func_accepts_config:\n\u001b[0;32m    605\u001b[0m         kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 606\u001b[0m     function_result \u001b[38;5;241m=\u001b[39m run_container[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mrun(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    608\u001b[0m     _on_run_end(run_container, error\u001b[38;5;241m=\u001b[39me)\n",
      "Cell \u001b[1;32mIn[2], line 42\u001b[0m, in \u001b[0;36mAgent.call_groq\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     39\u001b[0m messages \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconversation\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     41\u001b[0m conversation \u001b[38;5;241m=\u001b[39m [SystemMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msystem)] \u001b[38;5;241m+\u001b[39m messages\n\u001b[1;32m---> 42\u001b[0m ai_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconversation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconversation\u001b[39m\u001b[38;5;124m'\u001b[39m: [ai_response]}\n",
      "File \u001b[1;32mc:\\Users\\Dagher\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\runnables\\base.py:5094\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   5088\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m   5089\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5090\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[0;32m   5091\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   5092\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   5093\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[1;32m-> 5094\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m   5095\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   5096\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[0;32m   5097\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[0;32m   5098\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Dagher\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:291\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    287\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    288\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    290\u001b[0m         ChatGeneration,\n\u001b[1;32m--> 291\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    292\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    293\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    294\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    295\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    296\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    297\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    298\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    299\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    300\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    301\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32mc:\\Users\\Dagher\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:791\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    783\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    785\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    788\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    789\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    790\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Dagher\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:648\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    646\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[0;32m    647\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 648\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    649\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    650\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[0;32m    651\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[0;32m    652\u001b[0m ]\n\u001b[0;32m    653\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[1;32mc:\\Users\\Dagher\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:638\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[0;32m    636\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    637\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 638\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[0;32m    639\u001b[0m                 m,\n\u001b[0;32m    640\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    641\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    642\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    643\u001b[0m             )\n\u001b[0;32m    644\u001b[0m         )\n\u001b[0;32m    645\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    646\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\Dagher\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:860\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 860\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    861\u001b[0m             messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Dagher\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_groq\\chat_models.py:472\u001b[0m, in \u001b[0;36mChatGroq._generate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    467\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[0;32m    468\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    471\u001b[0m }\n\u001b[1;32m--> 472\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(messages\u001b[38;5;241m=\u001b[39mmessage_dicts, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[1;32mc:\\Users\\Dagher\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\groq\\resources\\chat\\completions.py:289\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    177\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    178\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    179\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;124;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/openai/v1/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    306\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Dagher\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\groq\\_base_client.py:1225\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1212\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1213\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1220\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1221\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1222\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1223\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1224\u001b[0m     )\n\u001b[1;32m-> 1225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\Dagher\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\groq\\_base_client.py:920\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    911\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    912\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    913\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    918\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    919\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 920\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Dagher\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\groq\\_base_client.py:1018\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1015\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1017\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1018\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1021\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1022\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1025\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1026\u001b[0m )\n",
      "\u001b[1;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '<function=query_graph\":{\"query\": \"MATCH (n:Occupation)-[]->(m:Personality_Trait) return n, m\"}</function>'}}"
     ]
    }
   ],
   "source": [
    "thread = {'configurable': {'thread_id': \"1\"}}\n",
    "output = agent.graph.stream({\"conversation\": [HumanMessage(content=\"Are these still present?\")], 'graph_data_to_be_used': []}, thread, stream_mode='values')\n",
    "\n",
    "for message in output:\n",
    "    message['conversation'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversation': [HumanMessage(content='Hi'),\n",
       "  AIMessage(content=\"Hello. I'm excited to help you discover some career paths that fit your personality. Let's get started. \\n\\nTo better understand your personality, I'm going to ask you a couple of questions based on the RAISEC model. Here's the first one:\\n\\n1. When working on a project, what do you usually focus on: the details and making sure everything is accurate, or the overall idea and how it can be developed?\", response_metadata={'token_usage': {'completion_tokens': 89, 'prompt_tokens': 686, 'total_tokens': 775, 'completion_time': 0.361075168, 'prompt_time': 0.198699464, 'queue_time': 0.006110654000000021, 'total_time': 0.559774632}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_5c5d1b5cfb', 'finish_reason': 'stop', 'logprobs': None}, id='run-93d76839-b57c-48c8-ae70-2472ff45327b-0', usage_metadata={'input_tokens': 686, 'output_tokens': 89, 'total_tokens': 775}),\n",
       "  HumanMessage(content='second option'),\n",
       "  AIMessage(content=\"You're likely someone who enjoys thinking big and exploring new ideas. That's great.\\n\\nHere's the second question:\\n\\n2. When interacting with others, do you tend to be more outgoing and sociable, enjoying the company of many people, or more reserved and reflective, preferring to work independently or in small groups?\", response_metadata={'token_usage': {'completion_tokens': 64, 'prompt_tokens': 786, 'total_tokens': 850, 'completion_time': 0.25715688, 'prompt_time': 0.225771853, 'queue_time': 0.005482714999999999, 'total_time': 0.482928733}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_b6828be2c9', 'finish_reason': 'stop', 'logprobs': None}, id='run-58497297-be7e-4de6-88f8-7f5187abf3f2-0', usage_metadata={'input_tokens': 786, 'output_tokens': 64, 'total_tokens': 850}),\n",
       "  HumanMessage(content='second option'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_kf48', 'function': {'arguments': '{\"query\": \"MATCH (n:Occupation)-[]->(m:Personality_Trait) return n, m\"}', 'name': 'query_graph'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 116, 'prompt_tokens': 861, 'total_tokens': 977, 'completion_time': 0.473482509, 'prompt_time': 0.269811279, 'queue_time': 0.005028068999999968, 'total_time': 0.743293788}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_b3ae7e594e', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-9ffbaa0b-52e9-49ef-b34f-b4b509d4dd04-0', tool_calls=[{'name': 'query_graph', 'args': {'query': 'MATCH (n:Occupation)-[]->(m:Personality_Trait) return n, m'}, 'id': 'call_kf48', 'type': 'tool_call'}], usage_metadata={'input_tokens': 861, 'output_tokens': 116, 'total_tokens': 977}),\n",
       "  AIMessage(content=\"Based on our conversation, I understand that you're someone who enjoys thinking big and exploring new ideas, and you tend to be more reserved and reflective, preferring to work independently or in small groups. Considering these traits, here are some suitable career paths for you:\\n\\n• **Barista**: While working as a barista, you'll have the opportunity to be creative with your drink-making skills, and you can work independently or in small teams. Your attention to detail will come in handy when crafting the perfect cup of coffee.\\n\\n• **Personal Care Aide**: In this role, you'll work one-on-one with clients, providing them with personal care and support. Your reserved nature will help you build strong, individual relationships with your clients.\\n\\n• **Childcare Worker**: Working with children can be a fun and creative experience. You'll have the opportunity to develop activities and engage with small groups of children, which suits your preference for working in smaller teams.\\n\\n• **Home Health Aide**: Similar to a personal care aide, you'll work one-on-one with clients in their homes, providing them with support and care. This role will allow you to work independently while still making a positive impact on people's lives.\\n\\n• **Amusement and Recreation Attendant**: In this role, you'll work behind the scenes to ensure that events and activities run smoothly. Your attention to detail will be valuable in this position, and you'll often work in small teams or independently.\\n\\n• **Dining Room and Cafeteria Attendant**: While not always the most glamorous job, working in food service can be a great way to work independently or in small teams while still interacting with people. You'll have the opportunity to be creative with food presentation and develop your attention to detail.\\n\\nThese careers may not be an exact match for your personality, but they offer a good balance of independence, creativity, and interaction with others.\", response_metadata={'token_usage': {'completion_tokens': 380, 'prompt_tokens': 1359, 'total_tokens': 1739, 'completion_time': 1.528254488, 'prompt_time': 0.368159919, 'queue_time': 0.005833611999999988, 'total_time': 1.896414407}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_b6828be2c9', 'finish_reason': 'stop', 'logprobs': None}, id='run-a854181a-9c58-4330-b8f7-8a9e600a00ac-0', usage_metadata={'input_tokens': 1359, 'output_tokens': 380, 'total_tokens': 1739}),\n",
       "  HumanMessage(content='Do you think I would make a good psychologist?'),\n",
       "  AIMessage(content=\"Being a psychologist can be a fantastic career for someone who is reserved and reflective, as it allows you to work one-on-one with clients and help them explore their thoughts and emotions. However, it does require strong interpersonal skills, empathy, and the ability to connect with people from diverse backgrounds.\\n\\nConsidering your preference for working independently or in small groups, you might excel in certain areas of psychology, such as research or academic psychology. However, if you're interested in clinical psychology or counseling, you may need to develop your interpersonal skills further to effectively connect with clients and build strong therapeutic relationships.\\n\\nThat being said, if you're passionate about psychology and willing to work on developing your interpersonal skills, you might find it a rewarding career. It's essential to remember that personality traits can evolve, and with practice and experience, you can grow into a role that might initially seem outside your comfort zone.\\n\\nWould you like to explore more career options or discuss what aspects of psychology interest you the most?\", response_metadata={'token_usage': {'completion_tokens': 197, 'prompt_tokens': 1298, 'total_tokens': 1495, 'completion_time': 0.792424945, 'prompt_time': 0.306489287, 'queue_time': 0.0053163689999999875, 'total_time': 1.098914232}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_b3ae7e594e', 'finish_reason': 'stop', 'logprobs': None}, id='run-3270e244-a5e3-40a8-a90e-363bb22e77db-0', usage_metadata={'input_tokens': 1298, 'output_tokens': 197, 'total_tokens': 1495}),\n",
       "  HumanMessage(content='I would like to know which psycholgy careers suite me? because there is more than one possibility, right?'),\n",
       "  AIMessage(content=\"There are many psychology careers that might suit you, given your preference for working independently or in small groups. Here are a few options to consider:\\n\\n1. **Research Psychologist**: As a research psychologist, you'll have the opportunity to design and conduct studies, collect data, and analyze results. This role often involves working independently or in small teams, and you can focus on topics that interest you the most.\\n\\n2. **Academic Psychologist**: If you enjoy teaching and sharing knowledge, you might consider a career as an academic psychologist. You'll have the opportunity to teach courses, develop curricula, and conduct research in a university setting.\\n\\n3. **Industrial-Organizational Psychologist**: Industrial-organizational psychologists apply psychological principles to improve performance and well-being in the workplace. This role might involve working with small teams or departments, and you can use your analytical skills to develop solutions.\\n\\n4. **Neuropsychologist**: Neuropsychologists study the relationship between the brain and behavior. This role often involves working with individuals or small groups, and you can focus on research or clinical applications.\\n\\n5. **Experimental Psychologist**: Experimental psychologists design and conduct studies to understand human behavior and cognition. This role often involves working independently or in small teams, and you can explore various topics, such as perception, attention, or memory.\\n\\nThese careers might be a good fit for you, but it's essential to remember that each role has its unique requirements and challenges. I recommend exploring each option further to see which one aligns best with your interests, skills, and values.\\n\\nWould you like me to suggest some resources for learning more about these careers or finding job opportunities in these fields?\", response_metadata={'token_usage': {'completion_tokens': 338, 'prompt_tokens': 1527, 'total_tokens': 1865, 'completion_time': 1.3667901040000001, 'prompt_time': 0.411144061, 'queue_time': 0.005446844000000006, 'total_time': 1.777934165}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_5c5d1b5cfb', 'finish_reason': 'stop', 'logprobs': None}, id='run-42b2dca7-5cda-40b5-ba4c-09cccc704d8b-0', usage_metadata={'input_tokens': 1527, 'output_tokens': 338, 'total_tokens': 1865}),\n",
       "  HumanMessage(content='Are this still present now?'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_gfhx', 'function': {'arguments': '{\"query\": \"MATCH (n:Occupation)-[]->(m:Personality_Trait) return n, m\"}', 'name': 'query_graph'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 1880, 'total_tokens': 1914, 'completion_time': 0.136, 'prompt_time': 0.502445641, 'queue_time': 0.005402335999999952, 'total_time': 0.638445641}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_9260b4bb2e', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-a44b8637-1673-4ba8-80e1-b6309ef916e2-0', tool_calls=[{'name': 'query_graph', 'args': {'query': 'MATCH (n:Occupation)-[]->(m:Personality_Trait) return n, m'}, 'id': 'call_gfhx', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1880, 'output_tokens': 34, 'total_tokens': 1914}),\n",
       "  AIMessage(content=\"Based on our conversation history and the extracted data, I've identified some suitable career tracks for you.\\n\\nYou mentioned earlier that you tend to be more reserved and reflective, preferring to work independently or in small groups. You also showed interest in exploring psychology careers.\\n\\nFrom the extracted data, I noticed that some occupations are related to multiple personality traits, which might suit your preferences. Here are some career tracks that might interest you:\\n\\n• **Personal Care Aides**: This occupation is related to Conventional, Social, and Realistic traits. As a personal care aide, you'll work one-on-one with clients, providing them with personal care and support. Your attention to detail and ability to work independently will be valuable in this role.\\n\\n• **Childcare Workers**: This occupation is related to Conventional, Social, and Artistic traits. As a childcare worker, you'll work with small groups of children, developing activities and engaging with them. Your creative side will be stimulated in this role, and you'll have the opportunity to work independently or in small teams.\\n\\n• **Baristas**: This occupation is related to Conventional, Social, and Realistic traits. As a barista, you'll work independently or in small teams, preparing drinks and interacting with customers. Your attention to detail and ability to work in a fast-paced environment will be valuable in this role.\\n\\n• **Dining Room and Cafeteria Attendants and Bartender Helpers**: This occupation is related to Conventional, Social, and Realistic traits. In this role, you'll work behind the scenes to ensure that events and activities run smoothly. Your attention to detail and ability to work independently or in small teams will be valuable in this role.\\n\\n• **Home Health Aides**: This occupation is related to Conventional, Social, and Realistic traits. As a home health aide, you'll work one-on-one with clients, providing them with support and care. Your ability to work independently and attention to detail will be valuable in this role.\\n\\n• **Amusement and Recreation Attendants**: This occupation is related to Conventional, Social, and Realistic traits. In this role, you'll work independently or in small teams, ensuring that events and activities run smoothly. Your attention to detail and ability to work in a fast-paced environment will be valuable in this role.\\n\\nThese career tracks might be a good fit for you, given your preferences and interests. Remember that each role has its unique requirements and challenges, so it's essential to research and explore each option further.\\n\\nWould you like me to suggest some resources for learning more about these careers or finding job opportunities in these fields?\", response_metadata={'token_usage': {'completion_tokens': 529, 'prompt_tokens': 2407, 'total_tokens': 2936, 'completion_time': 2.122780245, 'prompt_time': 0.572416682, 'queue_time': 0.005395114000000034, 'total_time': 2.695196927}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_5c5d1b5cfb', 'finish_reason': 'stop', 'logprobs': None}, id='run-83747080-4ab4-4c2f-99d6-bdb2484cceee-0', usage_metadata={'input_tokens': 2407, 'output_tokens': 529, 'total_tokens': 2936})],\n",
       " 'good_cypher_and_outputs': {'MATCH (n:Occupation)-[]->(m:Personality_Trait) return n, m': \"[{'n': {'title': 'Psychiatric_Aides'}, 'm': {'title': 'Conventional'}}, {'n': {'title': 'Physical_Therapist_Aides'}, 'm': {'title': 'Conventional'}}, {'n': {'title': 'Personal_Care_Aides'}, 'm': {'title': 'Conventional'}}, {'n': {'title': 'Hosts_and_Hostesses,_Restaurant,_Lounge,_and_Coffee_Shop'}, 'm': {'title': 'Conventional'}}, {'n': {'title': 'Home_Health_Aides'}, 'm': {'title': 'Conventional'}}, {'n': {'title': 'Childcare_Workers'}, 'm': {'title': 'Conventional'}}, {'n': {'title': 'Dining_Room_and_Cafeteria_Attendants_and_Bartender_Helpers'}, 'm': {'title': 'Conventional'}}, {'n': {'title': 'Baristas'}, 'm': {'title': 'Conventional'}}, {'n': {'title': 'Amusement_and_Recreation_Attendants'}, 'm': {'title': 'Conventional'}}, {'n': {'title': 'School_Bus_Monitors'}, 'm': {'title': 'Social'}}, {'n': {'title': 'Psychiatric_Aides'}, 'm': {'title': 'Social'}}, {'n': {'title': 'Physical_Therapist_Aides'}, 'm': {'title': 'Social'}}, {'n': {'title': 'Personal_Care_Aides'}, 'm': {'title': 'Social'}}, {'n': {'title': 'Nannies'}, 'm': {'title': 'Social'}}, {'n': {'title': 'Hosts_and_Hostesses,_Restaurant,_Lounge,_and_Coffee_Shop'}, 'm': {'title': 'Social'}}, {'n': {'title': 'Home_Health_Aides'}, 'm': {'title': 'Social'}}, {'n': {'title': 'Childcare_Workers'}, 'm': {'title': 'Social'}}, {'n': {'title': 'Dining_Room_and_Cafeteria_Attendants_and_Bartender_Helpers'}, 'm': {'title': 'Social'}}, {'n': {'title': 'Baristas'}, 'm': {'title': 'Social'}}, {'n': {'title': 'Amusement_and_Recreation_Attendants'}, 'm': {'title': 'Social'}}, {'n': {'title': 'Physical_Therapist_Aides'}, 'm': {'title': 'Realistic'}}, {'n': {'title': 'Personal_Care_Aides'}, 'm': {'title': 'Realistic'}}, {'n': {'title': 'Home_Health_Aides'}, 'm': {'title': 'Realistic'}}, {'n': {'title': 'Dining_Room_and_Cafeteria_Attendants_and_Bartender_Helpers'}, 'm': {'title': 'Realistic'}}, {'n': {'title': 'Baristas'}, 'm': {'title': 'Realistic'}}, {'n': {'title': 'Amusement_and_Recreation_Attendants'}, 'm': {'title': 'Realistic'}}, {'n': {'title': 'Nannies'}, 'm': {'title': 'Artistic'}}, {'n': {'title': 'Childcare_Workers'}, 'm': {'title': 'Artistic'}}, {'n': {'title': 'Hosts_and_Hostesses,_Restaurant,_Lounge,_and_Coffee_Shop'}, 'm': {'title': 'Enterprising'}}, {'n': {'title': 'Psychiatric_Aides'}, 'm': {'title': 'Investigative'}}]\"},\n",
       " 'bad_cypher': [],\n",
       " 'extracted_data': [\"[['Psychiatric_Aides','has_trait','Conventional'],['Psychiatric_Aides','has_trait','Social'],['Psychiatric_Aides','has_trait','Investigative'],['Physical_Therapist_Aides','has_trait','Conventional'],['Physical_Therapist_Aides','has_trait','Social'],['Physical_Therapist_Aides','has_trait','Realistic'],['Personal_Care_Aides','has_trait','Conventional'],['Personal_Care_Aides','has_trait','Social'],['Personal_Care_Aides','has_trait','Realistic'],['Hosts_and_Hostesses,_Restaurant,_Lounge,_and_Coffee_Shop','has_trait','Conventional'],['Hosts_and_Hostesses,_Restaurant,_Lounge,_and_Coffee_Shop','has_trait','Social'],['Hosts_and_Hostesses,_Restaurant,_Lounge,_and_Coffee_Shop','has_trait','Enterprising'],['Home_Health_Aides','has_trait','Conventional'],['Home_Health_Aides','has_trait','Social'],['Home_Health_Aides','has_trait','Realistic'],['Childcare_Workers','has_trait','Conventional'],['Childcare_Workers','has_trait','Social'],['Childcare_Workers','has_trait','Artistic'],['Dining_Room_and_Cafeteria_Attendants_and_Bartender_Helpers','has_trait','Conventional'],['Dining_Room_and_Cafeteria_Attendants_and_Bartender_Helpers','has_trait','Social'],['Dining_Room_and_Cafeteria_Attendants_and_Bartender_Helpers','has_trait','Realistic'],['Baristas','has_trait','Conventional'],['Baristas','has_trait','Social'],['Baristas','has_trait','Realistic'],['Amusement_and_Recreation_Attendants','has_trait','Conventional'],['Amusement_and_Recreation_Attendants','has_trait','Social'],['Amusement_and_Recreation_Attendants','has_trait','Realistic'],['Nannies','has_trait','Social'],['Nannies','has_trait','Artistic'],['School_Bus_Monitors','has_trait','Social']]\",\n",
       "  \"[['Psychiatric_Aides','related_to','Conventional'], ['Physical_Therapist_Aides','related_to','Conventional'], ['Personal_Care_Aides','related_to','Conventional'], ['Hosts_and_Hostesses,_Restaurant,_Lounge,_and_Coffee_Shop','related_to','Conventional'], ['Home_Health_Aides','related_to','Conventional'], ['Childcare_Workers','related_to','Conventional'], ['Dining_Room_and_Cafeteria_Attendants_and_Bartender_Helpers','related_to','Conventional'], ['Baristas','related_to','Conventional'], ['Amusement_and_Recreation_Attendants','related_to','Conventional'], ['School_Bus_Monitors','related_to','Social'], ['Psychiatric_Aides','related_to','Social'], ['Physical_Therapist_Aides','related_to','Social'], ['Personal_Care_Aides','related_to','Social'], ['Nannies','related_to','Social'], ['Hosts_and_Hostesses,_Restaurant,_Lounge,_and_Coffee_Shop','related_to','Social'], ['Home_Health_Aides','related_to','Social'], ['Childcare_Workers','related_to','Social'], ['Dining_Room_and_Cafeteria_Attendants_and_Bartender_Helpers','related_to','Social'], ['Baristas','related_to','Social'], ['Amusement_and_Recreation_Attendants','related_to','Social'], ['Physical_Therapist_Aides','related_to','Realistic'], ['Personal_Care_Aides','related_to','Realistic'], ['Home_Health_Aides','related_to','Realistic'], ['Dining_Room_and_Cafeteria_Attendants_and_Bartender_Helpers','related_to','Realistic'], ['Baristas','related_to','Realistic'], ['Amusement_and_Recreation_Attendants','related_to','Realistic'], ['Nannies','related_to','Artistic'], ['Childcare_Workers','related_to','Artistic'], ['Hosts_and_Hostesses,_Restaurant,_Lounge,_and_Coffee_Shop','related_to','Enterprising'], ['Psychiatric_Aides','related_to','Investigative']]\"],\n",
       " 'graph_data_to_be_used': ['MATCH (n:Occupation)-[]->(m:Personality_Trait) return n, m']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# agent.graph.get_state(thread).values['conversation'][-1].tool_calls[0]['args']['query']\n",
    "# agent.graph.get_state(thread).values['conversation']\n",
    "# agent.graph.get_state(thread).values\n",
    "\n",
    "state = agent.graph.get_state(thread).values\n",
    "state['good_cypher_and_outputs']\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tool_calls = state['conversation'][-2].tool_calls\n",
    "\n",
    "# good_cypher_and_outputs = {} # stores the cypher queries that returned an output\n",
    "# bad_cypher = [] # stores the cypher queries that did not return an output\n",
    "# graph_data_to_be_used = [] # stores the queries that the model currently wants to use\n",
    "\n",
    "# # Go over the cypher codes given by the LLM\n",
    "# for i in range(len(tool_calls)):\n",
    "#     current_tool_call = tool_calls[i]\n",
    "#     new_cypher = current_tool_call['args']['query']\n",
    "\n",
    "#     status = None\n",
    "\n",
    "#     # Check if tool exists\n",
    "#     if current_tool_call['name'] in agent.tools:\n",
    "        \n",
    "#         # LLM checks if the cypher query has already been made + if it returned an output\n",
    "#         if status == None:\n",
    "#             print(f\"----- Checker 1\")\n",
    "#             good_cyphers = list(state['good_cypher_and_outputs'].keys())\n",
    "#             for i in range(len( good_cyphers )):\n",
    "#                 key = good_cyphers[i]\n",
    "#                 comparison = agent.model.invoke(\n",
    "#                     prompts.cypher_code_analyst_prompt.format(cypher_code_1=new_cypher, cypher_code_2=key, graph_schema=graph.structured_schema)\n",
    "#                     )\n",
    "#                 print(f\"\\n-------- {comparison.content}\")\n",
    "#                 # If Query exists => Save cypher code to later access the output and give it to the model\n",
    "#                 if comparison.content.lower() == \"true\":\n",
    "#                     print(\"\\n----- Good Cyphers are similar, save the cypher\")\n",
    "#                     graph_data_to_be_used.append(key)\n",
    "#                     status = \"good_cypher\"\n",
    "#                     break\n",
    "        \n",
    "#         # LLM checks if the cypher query has already been made + if it didn't return an output\n",
    "#         if status == None:\n",
    "#             print(f\"----- Checker 2\")\n",
    "#             for i in range(len(state['bad_cypher'])):\n",
    "#                 comparison = agent.model.invoke(\n",
    "#                     prompts.cypher_code_analyst_prompt.format(\n",
    "#                         cypher_code_1=new_cypher, cypher_code_2=state['bad_cypher'][i], graph_schema=graph.structured_schema)\n",
    "#                     )\n",
    "#                 # Query exists => LLM should fix the query\n",
    "#                 if comparison.content.lower() == \"true\":\n",
    "#                     print(\"---- Bad Cyphers are similar, have the LLM query again\")\n",
    "#                     status = \"bad_cypher\"\n",
    "#                     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(comparison.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_knowledge_graph(data):\n",
    "    head = []\n",
    "    tail = []\n",
    "\n",
    "    # Extract head and tail nodes\n",
    "    for i in range(len(data)):\n",
    "        row = data[i]\n",
    "        keys = list(row.keys())\n",
    "        node_1 = row[keys[0]]['title']\n",
    "        node_2 = row[keys[1]]['title']\n",
    "\n",
    "        head.append(node_1)\n",
    "        tail.append(node_2)\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame({'head': head, 'tail': tail})\n",
    "\n",
    "    # Create the graph\n",
    "    G = nx.Graph()\n",
    "    for _, row in df.iterrows():\n",
    "        G.add_edge(row['head'], row['tail'], label=\"\")\n",
    "\n",
    "    # Get positions for nodes\n",
    "    pos = nx.fruchterman_reingold_layout(G, k=0.7)\n",
    "\n",
    "    # Create edge traces (lines between nodes)\n",
    "    edge_traces = []\n",
    "    for edge in G.edges():\n",
    "        x0, y0 = pos[edge[0]]\n",
    "        x1, y1 = pos[edge[1]]\n",
    "        edge_trace = go.Scatter(\n",
    "            x=[x0, x1, None],\n",
    "            y=[y0, y1, None],\n",
    "            mode='lines',\n",
    "            line=dict(width=0.3, color='gray'),\n",
    "            hoverinfo='none'\n",
    "        )\n",
    "        edge_traces.append(edge_trace)\n",
    "\n",
    "    # Assign colors based on whether the node is in the head or tail\n",
    "    node_colors = []\n",
    "    for node in G.nodes():\n",
    "        if node in head:\n",
    "            node_colors.append('lightblue')  # Color for head nodes (node_1)\n",
    "        elif node in tail:\n",
    "            node_colors.append('lightcoral')   # Color for tail nodes (node_2)\n",
    "        else:\n",
    "            node_colors.append('gold')  # Default color\n",
    "\n",
    "    # Create node trace (nodes with their respective colors)\n",
    "    node_trace = go.Scatter(\n",
    "        x=[pos[node][0] for node in G.nodes()],\n",
    "        y=[pos[node][1] for node in G.nodes()],\n",
    "        mode='markers+text',\n",
    "        marker=dict(size=10, color=node_colors),  # Use node_colors list for colors\n",
    "        text=[node for node in G.nodes()],\n",
    "        textposition='top center',\n",
    "        hoverinfo='text',\n",
    "        textfont=dict(size=7)\n",
    "    )\n",
    "\n",
    "    # Create edge label trace (optional, for labeling edges)\n",
    "    edge_label_trace = go.Scatter(\n",
    "        x=[(pos[edge[0]][0] + pos[edge[1]][0]) / 2 for edge in G.edges()],\n",
    "        y=[(pos[edge[0]][1] + pos[edge[1]][1]) / 2 for edge in G.edges()],\n",
    "        mode='text',\n",
    "        text=[G[edge[0]][edge[1]]['label'] for edge in G.edges()],\n",
    "        textposition='middle center',\n",
    "        hoverinfo='none',\n",
    "        textfont=dict(size=7)\n",
    "    )\n",
    "\n",
    "    # Create layout\n",
    "    layout = go.Layout(\n",
    "        title='',\n",
    "        titlefont_size=16,\n",
    "        title_x=0.5,\n",
    "        showlegend=False,\n",
    "        hovermode='closest',\n",
    "        margin=dict(b=20, l=5, r=5, t=40),\n",
    "        xaxis_visible=False,\n",
    "        yaxis_visible=False\n",
    "    )\n",
    "\n",
    "    # Create Plotly figure\n",
    "    fig = go.Figure(data=edge_traces + [node_trace, edge_label_trace], layout=layout)\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "# for i in range(len(state['graph_data_to_be_used'])):\n",
    "g = ast.literal_eval(state['good_cypher_and_outputs']['MATCH (n:Occupation)-[]->(m:Personality_Trait) RETURN n, m'])\n",
    "# display_knowledge_graph(g)\n",
    "g "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
