{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach 2, second try:\n",
    "- Scientist receives conversation. \n",
    "- Goes through the conversational flow.\n",
    "- Queries the graph.\n",
    "- Save query in history of queries. \n",
    "- Save Query results in history of query results. \n",
    "- ToolMessage is not saved in conversation messages.\n",
    "- Have a variable for the extracted occupations or traits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from langchain.graphs import Neo4jGraph\n",
    "from langchain_core.tools import tool\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# LangGraph\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# LangSmith\n",
    "from langsmith import traceable\n",
    "\n",
    "# General Imports\n",
    "import os\n",
    "import ast\n",
    "import prompts\n",
    "import operator\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from typing import TypedDict, Annotated # to construct the agent's state\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Connect to graph\n",
    "dotenv_path = Path('../.env')\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "os.environ[\"NEO4J_URI\"] = os.getenv('NEO4J_URI')\n",
    "os.environ[\"NEO4J_USERNAME\"] = os.getenv('NEO4J_USERNAME')\n",
    "os.environ[\"NEO4J_PASSWORD\"] = os.getenv('NEO4J_PASSWORD')\n",
    "graph = Neo4jGraph()\n",
    "\n",
    "# Create the tool to be used by the Agent\n",
    "@tool\n",
    "def query_graph(query):\n",
    "  \"\"\"Query from Neo4j knowledge graph using Cypher.\"\"\"\n",
    "  return graph.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Agent's State\n",
    "class AgentState(TypedDict):\n",
    "    conversation: Annotated[list[ AnyMessage ], operator.add]\n",
    "    good_cypher_and_outputs: Annotated[dict[ str, str ], operator.or_]\n",
    "    bad_cypher: Annotated[list[ str ], operator.add]\n",
    "    extracted_data: Annotated[list[ str ], operator.add]\n",
    "\n",
    "    # existing_cyphers: list[str]\n",
    "    graph_data_to_be_used: list[str]\n",
    "\n",
    "# Create Agent\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, model, tools, system: str):\n",
    "\n",
    "        graph = StateGraph(AgentState) # Initialize a stateful graph\n",
    "        memory = MemorySaver()\n",
    "\n",
    "        graph.add_node(\"personality_scientist\", self.call_groq)\n",
    "        graph.add_node(\"validate_cypher_then_query_graph\", self.validate_cypher_then_query_graph) # Checks if query is new\n",
    "        graph.add_node(\"extract_data\", self.extract_data)\n",
    "        graph.add_node(\"recommend_careers\", self.recommend_careers)\n",
    "\n",
    "        graph.add_conditional_edges(\"personality_scientist\", self.validate_tool_call, {True: 'validate_cypher_then_query_graph', False: END})\n",
    "        graph.add_edge(\"validate_cypher_then_query_graph\", \"extract_data\")\n",
    "        graph.add_edge(\"extract_data\", \"recommend_careers\")\n",
    "        graph.add_edge(\"recommend_careers\", END)\n",
    "        graph.set_entry_point(\"personality_scientist\")\n",
    "\n",
    "        self.graph = graph.compile(checkpointer=memory)\n",
    "        self.system = system\n",
    "        self.tools = {t.name: t for t in tools} # Save the tools' names that can be used\n",
    "        self.model = model.bind_tools(tools)\n",
    "\n",
    "    ## Helper function that returns the cyphers written before. This is used when calling the LLM\n",
    "    def get_previous_cyphers(self, state: AgentState):\n",
    "        cyphers_list = \"\"\n",
    "        \n",
    "        if len(state['bad_cypher']) > 0:\n",
    "            cyphers_list += f\"Here are previously written cypher codes that did not return an output: {str(state['bad_cypher'])}\"\n",
    "\n",
    "        good_cypher = list(state['good_cypher_and_outputs'].keys())\n",
    "        if len(good_cypher) > 0:\n",
    "            cyphers_list += f\"Here are previously written cypher codes that successfully returned an output: {str(good_cypher)}\"\n",
    "        \n",
    "        if cyphers_list != \"\": \n",
    "            output = \"When you are ready to write the cypher code, use these as help:\" + cyphers_list\n",
    "            return output\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    ## Get the LLM's response and update the Agent's State by adding the response to the messages\n",
    "    @traceable\n",
    "    def call_groq(self, state: AgentState):\n",
    "        previous_cyphers = self.get_previous_cyphers(state = state)\n",
    "        if previous_cyphers:\n",
    "            system_message = f\"{self.system}. {previous_cyphers}\"\n",
    "        else:\n",
    "            system_message = self.system\n",
    "\n",
    "        conversation = [SystemMessage(content=system_message)] + state['conversation']\n",
    "        ai_response = self.model.invoke(conversation)\n",
    "        return {'conversation': [ai_response]}\n",
    "    \n",
    "    ## Check if the model called for an action by checking the last message in the conversation\n",
    "    def validate_tool_call(self, state: AgentState):\n",
    "        ai_message = state['conversation'][-1]\n",
    "        return len(ai_message.tool_calls) > 0\n",
    "    \n",
    "    ## Check if the new cypher codes have already been written by the LLM\n",
    "    def validate_cypher_then_query_graph(self, state: AgentState):\n",
    "        print(f\"\\n------- in validate query\")\n",
    "        tool_calls = state['conversation'][-1].tool_calls\n",
    "\n",
    "        good_cypher_and_outputs = {} # stores the cypher queries that returned an output\n",
    "        bad_cypher = [] # stores the cypher queries that did not return an output\n",
    "        graph_data_to_be_used = [] # stores the queries that the model currently wants to use\n",
    "\n",
    "        # Go over the cypher codes given by the LLM\n",
    "        for i in range(len(tool_calls)):\n",
    "            current_tool_call = tool_calls[i]\n",
    "            new_cypher = current_tool_call['args']['query']\n",
    "\n",
    "            status = None\n",
    "\n",
    "            # Check if tool exists\n",
    "            if current_tool_call['name'] in self.tools:\n",
    "                \n",
    "                # LLM checks if the cypher query has already been made + if it returned an output\n",
    "                if status == None:\n",
    "                    print(f\"----- Checker 1\")\n",
    "                    good_cyphers = list(state['good_cypher_and_outputs'].keys())\n",
    "                    for i in range(len( good_cyphers )):\n",
    "                        key = good_cyphers[i]\n",
    "                        comparison = self.model.invoke(\n",
    "                            prompts.cypher_code_analyst_prompt.format(cypher_code_1=new_cypher, cypher_code_2=key, graph_schema=graph.structured_schema)\n",
    "                            )\n",
    "                        print(f\"\\n-------- {comparison.content}\")\n",
    "                        # If Query exists => Save cypher code to later access the output and give it to the model\n",
    "                        if comparison.content.lower() == \"true\":\n",
    "                            print(\"\\n----- Good Cyphers are similar, save the cypher\")\n",
    "                            graph_data_to_be_used.append(key)\n",
    "                            status = \"good_cypher\"\n",
    "                            break\n",
    "                \n",
    "                # LLM checks if the cypher query has already been made + if it didn't return an output\n",
    "                if status == None:\n",
    "                    print(f\"----- Checker 2\")\n",
    "                    for i in range(len(state['bad_cypher'])):\n",
    "                        comparison = self.model.invoke(\n",
    "                            prompts.cypher_code_analyst_prompt.format(\n",
    "                                cypher_code_1=new_cypher, cypher_code_2=state['bad_cypher'][i], graph_schema=graph.structured_schema)\n",
    "                            )\n",
    "                        # Query exists => LLM should fix the query\n",
    "                        if comparison.content.lower() == \"true\":\n",
    "                            print(\"---- Bad Cyphers are similar, have the LLM query again\")\n",
    "                            status = \"bad_cypher\"\n",
    "                            break\n",
    "\n",
    "                # If the cypher code hasn't been used before => query the graph\n",
    "                if status == None:\n",
    "                    print(f\"----- Checker 3\")\n",
    "                    query_output = self.tools[current_tool_call['name']].invoke(new_cypher)\n",
    "                    result = ToolMessage(content=str(query_output), name=current_tool_call['name'], tool_call_id=current_tool_call['id'])\n",
    "                    \n",
    "                    if result.content not in [\"\", None, '[]']:\n",
    "                        good_cypher_and_outputs[new_cypher] = result.content\n",
    "                        graph_data_to_be_used.append(new_cypher)\n",
    "                        print(\"----- Successfully queried graph\")\n",
    "\n",
    "                    else:\n",
    "                        bad_cypher.append(new_cypher)\n",
    "                        print(\"----- Bad query\")\n",
    "            else:\n",
    "                print(\"tool name not found in list of tools\")\n",
    "\n",
    "        # Save the data that we got in the AgentState\n",
    "        return_statement = {}\n",
    "        if len(good_cypher_and_outputs) > 0: return_statement['good_cypher_and_outputs'] = good_cypher_and_outputs\n",
    "        if len(bad_cypher) > 0: return_statement['bad_cypher'] = bad_cypher\n",
    "        if len(graph_data_to_be_used) > 0: return_statement['graph_data_to_be_used'] = graph_data_to_be_used\n",
    "\n",
    "        return return_statement\n",
    "    \n",
    "    ## Invoke tool\n",
    "    def extract_data(self, state: AgentState):\n",
    "        print('\\n-------> In extract data')\n",
    "\n",
    "        if len(state['graph_data_to_be_used']) > 0:\n",
    "            data_to_give_to_the_LLM = [{cypher: state['good_cypher_and_outputs'][cypher]} for cypher in state['graph_data_to_be_used']]\n",
    "            extracted_data = self.model.invoke(prompts.extractor_prompt.format(queried_data = data_to_give_to_the_LLM))\n",
    "            print(\"----- Data has been extracted\")\n",
    "            return {'extracted_data': [extracted_data.content]}\n",
    "\n",
    "        else:\n",
    "            print(\"No queries to be made\")\n",
    "            return\n",
    "\n",
    "    ## Generate final output\n",
    "    def recommend_careers(self, state: AgentState):\n",
    "        previous_cyphers = self.get_previous_cyphers(state = state)\n",
    "        if previous_cyphers:\n",
    "            system_message = f\"{self.system}. {previous_cyphers}\"\n",
    "        else:\n",
    "            system_message = self.system\n",
    "        \n",
    "        conversation = [SystemMessage(content=system_message)] + state['conversation']\n",
    "\n",
    "        if len(state['graph_data_to_be_used']) > 0:\n",
    "            prompt = conversation + [HumanMessage(content= prompts.recommender_prompt_with_data.format(extracted_data=state['extracted_data'][-1]))]\n",
    "            print(\"----- Giving extracted data to the agent\")\n",
    "            \n",
    "            recommended_careers = self.model.invoke(prompt)\n",
    "            print(\"ready for recommendation\")\n",
    "            return {'conversation': [recommended_careers]}\n",
    "        else:\n",
    "            \n",
    "            prompt = conversation + [HumanMessage(content= prompts.recommender_prompt_without_data)]\n",
    "            print(\"----- No data to give to the agent\")\n",
    "            \n",
    "            ai_output = self.model.invoke(prompt)\n",
    "            print(\"ready for recommendation\")\n",
    "            return {'conversation': [ai_output]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGroq(temperature=0.8, groq_api_key=os.environ[\"GROQ_API_KEY\"], model_name=\"llama-3.1-70b-versatile\", max_retries=2)\n",
    "# model = ChatGroq(temperature=0.0, groq_api_key=os.environ[\"GROQ_API_KEY\"], model_name=\"llama3-70b-8192\")\n",
    "\n",
    "agent = Agent(\n",
    "  model=model, \n",
    "  tools=[query_graph], \n",
    "  system=prompts.personality_scientist_prompt.format(schema=graph.structured_schema)\n",
    "  )\n",
    "\n",
    "# graph_png = Image(agent.graph.get_graph().draw_mermaid_png())\n",
    "# display(graph_png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = {'configurable': {'thread_id': \"100\"}}\n",
    "output = agent.graph.stream({\"conversation\": [HumanMessage(content=\"try to analyze it from different angles to find a logical solution\")], 'graph_data_to_be_used': []}, thread, stream_mode='values')\n",
    "\n",
    "for message in output:\n",
    "    message['conversation'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = agent.graph.get_state(thread).values\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_knowledge_graph(data):\n",
    "    head = []\n",
    "    tail = []\n",
    "\n",
    "    # Extract head and tail nodes\n",
    "    for i in range(len(data)):\n",
    "        row = data[i]\n",
    "        keys = list(row.keys())\n",
    "        node_1 = row[keys[0]]['title']\n",
    "        node_2 = row[keys[1]]['title']\n",
    "\n",
    "        head.append(node_1)\n",
    "        tail.append(node_2)\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame({'head': head, 'tail': tail})\n",
    "\n",
    "    # Create the graph\n",
    "    G = nx.Graph()\n",
    "    for _, row in df.iterrows():\n",
    "        G.add_edge(row['head'], row['tail'], label=\"\")\n",
    "\n",
    "    # Get positions for nodes\n",
    "    pos = nx.fruchterman_reingold_layout(G, k=0.7)\n",
    "\n",
    "    # Create edge traces (lines between nodes)\n",
    "    edge_traces = []\n",
    "    for edge in G.edges():\n",
    "        x0, y0 = pos[edge[0]]\n",
    "        x1, y1 = pos[edge[1]]\n",
    "        edge_trace = go.Scatter(\n",
    "            x=[x0, x1, None],\n",
    "            y=[y0, y1, None],\n",
    "            mode='lines',\n",
    "            line=dict(width=0.3, color='gray'),\n",
    "            hoverinfo='none'\n",
    "        )\n",
    "        edge_traces.append(edge_trace)\n",
    "\n",
    "    # Assign colors based on whether the node is in the head or tail\n",
    "    node_colors = []\n",
    "    for node in G.nodes():\n",
    "        if node in head:\n",
    "            node_colors.append('lightblue')  # Color for head nodes (node_1)\n",
    "        elif node in tail:\n",
    "            node_colors.append('lightcoral')   # Color for tail nodes (node_2)\n",
    "        else:\n",
    "            node_colors.append('gold')  # Default color\n",
    "\n",
    "    # Create node trace (nodes with their respective colors)\n",
    "    node_trace = go.Scatter(\n",
    "        x=[pos[node][0] for node in G.nodes()],\n",
    "        y=[pos[node][1] for node in G.nodes()],\n",
    "        mode='markers+text',\n",
    "        marker=dict(size=10, color=node_colors),  # Use node_colors list for colors\n",
    "        text=[node for node in G.nodes()],\n",
    "        textposition='top center',\n",
    "        hoverinfo='text',\n",
    "        textfont=dict(size=7)\n",
    "    )\n",
    "\n",
    "    # Create edge label trace (optional, for labeling edges)\n",
    "    edge_label_trace = go.Scatter(\n",
    "        x=[(pos[edge[0]][0] + pos[edge[1]][0]) / 2 for edge in G.edges()],\n",
    "        y=[(pos[edge[0]][1] + pos[edge[1]][1]) / 2 for edge in G.edges()],\n",
    "        mode='text',\n",
    "        text=[G[edge[0]][edge[1]]['label'] for edge in G.edges()],\n",
    "        textposition='middle center',\n",
    "        hoverinfo='none',\n",
    "        textfont=dict(size=7)\n",
    "    )\n",
    "\n",
    "    # Create layout\n",
    "    layout = go.Layout(\n",
    "        title='',\n",
    "        titlefont_size=16,\n",
    "        title_x=0.5,\n",
    "        showlegend=False,\n",
    "        hovermode='closest',\n",
    "        margin=dict(b=20, l=5, r=5, t=40),\n",
    "        xaxis_visible=False,\n",
    "        yaxis_visible=False\n",
    "    )\n",
    "\n",
    "    # Create Plotly figure\n",
    "    fig = go.Figure(data=edge_traces + [node_trace, edge_label_trace], layout=layout)\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "# for i in range(len(state['graph_data_to_be_used'])):\n",
    "g = ast.literal_eval(state['good_cypher_and_outputs']['MATCH (n:Occupation)-[]->(m:Personality_Trait) RETURN n, m'])\n",
    "# display_knowledge_graph(g)\n",
    "g "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
