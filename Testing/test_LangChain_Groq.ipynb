{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "from langchain.vectorstores.neo4j_vector import Neo4jVector\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get environment variables\n",
    "dotenv_path = Path('./.env')\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "os.environ[\"NEO4J_URI\"] = os.getenv('uri')\n",
    "os.environ[\"NEO4J_USERNAME\"] = os.getenv('user_name')\n",
    "os.environ[\"NEO4J_PASSWORD\"] = os.getenv('password')\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading a PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load a sample PDF\n",
    "# loader = PyPDFLoader(file_path=\"Project Proposal - Career Guide and Recommender System.pdf\")\n",
    "# pages = loader.load_and_split()\n",
    "# raw_pages = [pages[i].page_content for i in range(len(pages))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading an Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# # Load an embedding model to create embeddings\n",
    "# embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# # Load tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# def bert_len(text):\n",
    "#     tokens = tokenizer.encode(text)\n",
    "#     return len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading and using Text Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split document into chunks\n",
    "# text_splitter = RecursiveCharacterTextSplitter(\n",
    "#           chunk_size = 200,\n",
    "#           chunk_overlap  = 20,\n",
    "#           length_function = bert_len,\n",
    "#           separators=['\\n\\n', '\\n', ' ', ''],\n",
    "#       )\n",
    "\n",
    "# documents = text_splitter.create_documents(raw_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Store vectorized documents in Neo4j database as vectors\n",
    "# neo4j_vector = Neo4jVector.from_documents( documents, embedding_model )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Perform a vector similarity search and retrieve the top 2 most similar documents\n",
    "# query = \"Who are the authors of this paper?\"\n",
    "\n",
    "# vector_results = neo4j_vector.similarity_search(query, k=2)\n",
    "\n",
    "# for i, res in enumerate(vector_results):\n",
    "#     print(res.page_content)\n",
    "#     if i != len(vector_results)-1:\n",
    "#         print()\n",
    "\n",
    "# vector_result = vector_results[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing GraphCypherQAChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Query Knowledge Graph\n",
    "from langchain.chains import GraphCypherQAChain\n",
    "from langchain.graphs import Neo4jGraph\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Connect to graph\n",
    "graph = Neo4jGraph()\n",
    "\n",
    "# Create LLM instance\n",
    "llm = ChatGroq(temperature=0, groq_api_key=os.environ[\"GROQ_API_KEY\"], model_name=\"llama3-8b-8192\")\n",
    "\n",
    "graph_schema = f\"Context of the graph's schema: {graph.schema}\"\n",
    "personality_traits = graph.query(\"MATCH (n:Personality_Trait) return n\")\n",
    "personality_traits = [title['n']['title'] for title in personality_traits]\n",
    "\n",
    "label_and_prop = f\"Context of the Label and Properties of nodes: {graph.get_structured_schema['node_props']}\"\n",
    "rel = f\"Context of the properties of the Relationships between Nodes: {graph.get_structured_schema['rel_props']}\"\n",
    "rel_and_prop = f\"Context of the Relationships between Nodes{graph.get_structured_schema['relationships']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"Your role is to recommend career paths for me. You have access to a Neo4j Graph Database. Here is the graph's schema: {graph_schema} and here the Personality_Trait titles that you can use: {personality_traits} and not that job_zone resembles the level of experience required for the job from 1 till 5 as integers. Use the graph to supplement your answer. Here are some details about myself: I love people and I am a good listener. I enjoy observation and analysis. what careers would you recommend?\"\"\"\n",
    "\n",
    "chain = GraphCypherQAChain.from_llm(graph=graph, llm=llm, verbose=True, validate_cypher=True)\n",
    "response = chain.invoke({\"query\": prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.query(\"MATCH (o:Occupation)-[:need_for_personality_trait]->(pt:Personality_Trait) WHERE pt.title IN ['Social', 'Artistic'] RETURN o.title AS CareerPath\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Tool Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from graph_functions import connect_to_database\n",
    "\n",
    "class QueryKnowledgeGraph(BaseModel):\n",
    "    f'''The contains details about Occupations and their related demands. This is the schema of the graph as context: {label_and_prop, rel, rel_and_prop}. In your cypher query do not use any words that are not found in the schema. Use the graphs relations, labels, and properties to supplement your answer. Return an answer to the question, appraoch you used, the node_id that you referred to, and the query you wrote.'''\n",
    "\n",
    "    answer: str\n",
    "    approach_used: str\n",
    "    node_id: str\n",
    "    cypher_query: str\n",
    "\n",
    "    def query_neo4j(query):\n",
    "        result = graph.query(query)\n",
    "        return result\n",
    "\n",
    "\n",
    "chat = ChatGroq(temperature=1, groq_api_key=os.environ[\"GROQ_API_KEY\"], model_name=\"mixtral-8x7b-32768\")\n",
    "structured_llm = chat.with_structured_output(QueryKnowledgeGraph, include_raw=True)\n",
    "\n",
    "answer = structured_llm.invoke(\"I love people and I love to listen, what would you recommend as a career?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer['parsed'].answer)\n",
    "print(answer['parsed'].approach_used)\n",
    "print(answer['parsed'].node_id)\n",
    "print(answer['parsed'].cypher_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = connect_to_database(uri=os.getenv('uri'), username = os.getenv('user_name'), password = os.getenv('password'))\n",
    "query = f\"MATCH (n:Occupation)-[]-(m:`Personality Trait` {{title: 'Artistic'}}) RETURN n\"\n",
    "# with driver.session() as session:\n",
    "#     result = session.run(query)\n",
    "#     result = result.value()\n",
    "#     driver.close()\n",
    "result = graph.query(query)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually Use Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tetsing groq alone\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(api_key=os.environ[\"GROQ_API_KEY\"])\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Introduce yourself\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-groq-70b-8192-tool-use-preview\"\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://download.llamameta.net/*?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoienhzcW5qdnJwdWYyZDRwMnB2N2Jydzg4IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMzc4OTY5NX19fV19&Signature=SndIBApuNRCYZknzXVDyzXwL083ASH6kq42bpq-j8IHM6LRwPwK8456EmYa-XNjedbJNeDPpjdpxo%7EZtMe%7EVoDz4MUep-AwmhnyRlIim9-nwOZOGTpp%7El4bNUBRB%7Et8ARUZiQ%7EMdQa59BgNvmxQuNSAiwkYx0kr5pxV%7EUmqcTdHLM5B2FfGvCVfQrwIMwcXGYJI5SSwMH-4byp6ZSgbJM50WtykKpnyukLsa6kybY38AbUHxI5BdMeYpIPM9rIGlknWOEPaZE8oTnCeIf0NwEyN8fG3F4sFThEKN6KA2r0ilV3z2TkGKu6MpcnvvtdIMPldSlG-ewyUdHRGUWK98kg__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=2562051127315960"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_and_prop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "chat = ChatGroq(temperature=0, groq_api_key=os.environ[\"GROQ_API_KEY\"], model_name=\"mixtral-8x7b-32768\")\n",
    "\n",
    "prompt = f\"The neo4j graph database contains details about Occupations and their related demands. This is the schema of the graph as context:\\n{label_and_prop}\\n{rel}\\n{rel_and_prop}\\nIn your cypher query do not use any words that are not found in the schema. Use the graphs relations, labels, and properties to supplement your answer. Return an answer to the question, appraoch you used, the node_id that you referred to, and the query you wrote.\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(prompt)\n",
    "\n",
    "response = chat.invoke(\"I love people and I love to listen, what would you recommend as a career?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding memory to a conversation\n",
    "\n",
    "from langchain.chains import ConversationChain # Depricated\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory\n",
    "\n",
    "llm = ChatGroq(temperature=0, groq_api_key=os.environ[\"GROQ_API_KEY\"], model_name=\"mixtral-8x7b-32768\")\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "conversation = ConversationChain(\n",
    "  llm=llm,\n",
    "  memory=memory,\n",
    "  verbose=True\n",
    ")\n",
    "\n",
    "# To manually load previous memory\n",
    "# memory.save_context({\"input\": \"Hi\"}, {\"output\": \"What's up\"})\n",
    "# memory.load_memory_variables({})\n",
    "# conversation.predict(input=\"Limit your answers to one line. Give me a  love poem.\")\n",
    "# print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A chain of prompts returning one output from one input\n",
    "from langchain.chains import SequentialChain, LLMChain\n",
    "\n",
    "llm = ChatGroq(temperature=0.9, groq_api_key=os.environ[\"GROQ_API_KEY\"], model_name=\"llama-3.1-70b-versatile\")\n",
    "\n",
    "# prompt template 1: translate to english\n",
    "first_prompt = ChatPromptTemplate.from_template(\"Translate the following review to english:\\n\\n{Review}\")\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt, output_key=\"English_Review\") # chain 1: input= Review and output= English_Review\n",
    "\n",
    "second_prompt = ChatPromptTemplate.from_template(\"Can you summarize the following review in 1 sentence:\\n\\n{English_Review}\")\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt, output_key=\"summary\") # chain 2: input= English_Review and output= summary\n",
    "\n",
    "# prompt template 3: translate to english\n",
    "third_prompt = ChatPromptTemplate.from_template(\"What language is the following review:\\n\\n{Review}\")\n",
    "chain_three = LLMChain(llm=llm, prompt=third_prompt, output_key=\"language\") # chain 3: input= Review and output= language\n",
    "\n",
    "# prompt template 4: follow up message\n",
    "fourth_prompt = ChatPromptTemplate.from_template(\"Write a follow up response to the following summary in the specified language:\\n\\nSummary: {summary}\\n\\nLanguage: {language}\")\n",
    "chain_four = LLMChain(llm=llm, prompt=fourth_prompt, output_key=\"followup_message\") # chain 4: input= summary, language and output= followup_message\n",
    "\n",
    "# overall_chain: input= Review \n",
    "# and output= English_Review,summary, followup_message\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_one, chain_two, chain_three, chain_four],\n",
    "    input_variables=[\"Review\"], \n",
    "    output_variables=[\"English_Review\", \"summary\",\"followup_message\"], \n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = overall_chain.invoke(\"Ana b7ebbak men kel albe ya kbir\")\n",
    "\n",
    "print(\"review => \", output['Review'], '\\n')\n",
    "print(\"english review => \", output['English_Review'], '\\n')\n",
    "print(\"summary => \", output['summary'], '\\n')\n",
    "print(\"followup message => \", output['followup_message'], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LangChain Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I couldn't find any information on a well-known person named Marc Dagher. It's possible that Marc Dagher is a private individual or not a public figure. Can you provide more context or details about who Marc Dagher is or what he is known for? This will help me provide a more accurate answer."
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "model = ChatGroq(temperature=0, groq_api_key=os.environ[\"GROQ_API_KEY\"], model_name=\"llama-3.1-70b-versatile\")\n",
    "for chunk in model.stream(\"Who is Marc Dagher?\"):\n",
    "  print(chunk.content, end = \"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
