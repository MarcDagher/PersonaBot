{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Get environment variables\n",
    "dotenv_path = Path('./.env')\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "os.environ[\"NEO4J_URI\"] = os.getenv('uri')\n",
    "os.environ[\"NEO4J_USERNAME\"] = os.getenv('user_name')\n",
    "os.environ[\"NEO4J_PASSWORD\"] = os.getenv('password')\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv('GROQ_API_KEY')\n",
    "os.environ[\"TAVILY_API_KEY\"] = os.getenv('TAVILY_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated # to construct the agent's state\n",
    "import operator\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add] # operator.add allows us to add messages instead of replacing them when the LLM's output is returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self, model, tools, system=\"\"):\n",
    "        self.system = system\n",
    "        graph = StateGraph(AgentState) # initialize a stateful graph\n",
    "        memory = MemorySaver() # initialize\n",
    "\n",
    "        graph.add_node(\"llm\", self.call_groq) # Add LLM node\n",
    "        graph.add_node(\"action\", self.take_action) # Add Tool node\n",
    "        \n",
    "        # The edge where the decision to use a tool is made\n",
    "        graph.add_conditional_edges(\"llm\", self.exists_action, {True: \"action\", False: END})\n",
    "        \n",
    "        # Create edge and set starting point of the graph\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "        graph.set_entry_point(\"llm\")\n",
    "\n",
    "        self.graph = graph.compile(checkpointer=memory) # Build graph\n",
    "        \n",
    "        self.tools = {t.name: t for t in tools} # Save the tools' names that can be used\n",
    "        self.model = model.bind_tools(tools) # Provide the name of the tools to the agent\n",
    "\n",
    "    # Used by the Agent to check if an action exists by checking the last message in the state which is supposed to contain the tool's info\n",
    "    def exists_action(self, state: AgentState):\n",
    "        result = state['messages'][-1]\n",
    "        return len(result.tool_calls) > 0\n",
    "\n",
    "    # Get the LLM's response and update the Agent's State by adding the response to the messages\n",
    "    def call_groq(self, state: AgentState):\n",
    "        messages = state['messages']\n",
    "        if self.system: messages = [SystemMessage(content=self.system)] + messages\n",
    "        \n",
    "        message = self.model.invoke(messages)\n",
    "        return {'messages': [message]}\n",
    "\n",
    "    # Search for the tool and use it\n",
    "    def take_action(self, state: AgentState):\n",
    "        tool_calls = state['messages'][-1].tool_calls\n",
    "        results = []\n",
    "        for t in tool_calls:\n",
    "            print(f\"Calling: {t}\")\n",
    "            \n",
    "            # If tool not found\n",
    "            if not t['name'] in self.tools: \n",
    "                print(\"\\n ....tool name not found in list of tools....\")\n",
    "                result = \"bad tool name, retry\"  # instruct LLM to retry\n",
    "            \n",
    "            # If tool exists, use it\n",
    "            else:\n",
    "                result = self.tools[t['name']].invoke(t['args']) \n",
    "\n",
    "            # Save the message returned from the tool\n",
    "            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n",
    "\n",
    "        print(\"Back to the model!\")\n",
    "        return {'messages': results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tool to be used by the Agent\n",
    "from langchain.graphs import Neo4jGraph\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# Connect to graph\n",
    "graph = Neo4jGraph()\n",
    "\n",
    "@tool\n",
    "def query_graph(query):\n",
    "  \"\"\"Requires get_graph_schema to be run before using this function. This function is to Query from Neo4j knowledge graph using Cypher.\"\"\"\n",
    "  return graph.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"Task: You are a career guide. Your job is to ask me up to 3 questions to uncover my personality traits according to the RAISEC model. You will ask these questions in a conversational flow where you will ask the second question after I answer the first. Once you understand my personality, you will stop asking questions and use a Neo4j database to improve your knowledge on compatible career paths for me. You will query the possible occupation titles that are suitable for my character. At any point, I can ask you questions and you will answer normally, then you will continue your personality test.\"\n",
    "\n",
    "goal = \"Understand my personality and then suggest suitable career paths. Note: when asking your questions, please number them to keep track of the number of questions asked.\"\n",
    "\n",
    "schema_context = f\"Here is the graph's schema: {graph.structured_schema}.\"\n",
    "\n",
    "property_values = f\"Property Values: empty\"\n",
    "\n",
    "query_approach = \"Querying approach: You will not use 'LIMIT'. If Property Values: empty, you will not use general queries and will not include 'WHERE' or try to specify property values inside your Cypher code.\"\n",
    "\n",
    "output = \"Your final output: Interpret all the queried data, choose up to 3 suitable careers for me, list them in bullet points and include a brief explanation of how each path suites my personality. Include Cypher code in your answer.\"\n",
    "\n",
    "tone = \"Output's tone: Make your output friendly, fun and easy to read.\"\n",
    "\n",
    "personal_info = \"Personal Info: I love people and I am a good listener. I enjoy observation and analysis. I prefer being with abults rather than with kids and I also have computer programming skills.\"\n",
    "\n",
    "reminder = \"Reminder: If Property Values: empty, you will not use 'WHERE' or try to specify property values inside your Cypher code. Under no circumstances should you use 'DELETE'. Find the occupations that suite my character.\"\n",
    "\n",
    "prompt = f\"{task}\\ {goal}\\ {schema_context}\\ {property_values}\\ {query_approach}\\ {output}\\ {tone}\\ {personal_info}\\ {reminder}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare agent and the initial prompt\n",
    "model = ChatGroq(temperature=0, groq_api_key=os.environ[\"GROQ_API_KEY\"], model_name=\"llama-3.1-70b-versatile\")\n",
    "abot = Agent(model, [query_graph], system=prompt)\n",
    "\n",
    "# Prepare Human message\n",
    "query = \"What will we do today\"\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "input_message = HumanMessage(content=query)\n",
    "for event in abot.graph.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "input_message = HumanMessage(content=\"I enjoy a balance between creative work, analysis, and teamwork.\")\n",
    "for event in abot.graph.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "input_message = HumanMessage(content=\"Reserved listener who observes\")\n",
    "for event in abot.graph.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "input_message = HumanMessage(content=\"When problem solving I mostly use intuition with some logic\")\n",
    "for event in abot.graph.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Visualize Graph\n",
    "from IPython.display import Image\n",
    "\n",
    "# Image(abot.graph.get_graph().draw_png())\n",
    "Image(abot.graph.get_graph().draw_mermaid_png(), width=300)\n",
    "# print(abot.graph.get_graph().draw_ascii())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
